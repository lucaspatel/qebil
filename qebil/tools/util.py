import hashlib
from os import makedirs, path, remove
import pandas as pd
import numpy as np
import PyPDF2
import re
import requests
from shutil import move
import urllib
from time import sleep

from qebil.tools.fastq import get_read_length, get_read_count

from qebil.log import logger

THIS_DIR, THIS_FILENAME = path.split(__file__)


def load_project_file(filepath):
    """Reads in a project file to populate a list of study ids

    This method looks for a 'study_id' column in a tsv-formatted
    text file, as generated by the search function of QEBIL, and
    checks the first column if not found. This latter catch allows
    the user to supply a list of project ids, one per line to fetch.

    Parameters
    ----------
    filepath: string:
        path to the project file with a 'study_id' column

    Returns
    ---------
    add_list: list
        list of EBI project IDs found in the file

    """
    proj_df = pd.read_csv(filepath, sep="\t", header=0)
    if "study_id" not in proj_df.columns:
        proj_df = pd.read_csv(filepath, sep="\t", header=None)
        proj_df.columns = ["study_id"]
    add_list = list(proj_df["study_id"].unique())
    add_list = [x.strip() for x in add_list]

    return add_list


def get_checksum(filepath, compare_md5=""):
    """Gets the checksum of a file, comparing with expected value if provided

    Obtains the checksum of a file and compares with the expected
    md5 checksum if provided. Returns False if they do not match
    or the checksum value if they do.

    Parameters
    ----------
    filepath: string
        the path to the file or URL to be scraped
    compare_md5: string
        expected md5 checksum value

    Returns
    ---------
    False OR local_checksum:string
        checksum of local file if valid, otherwise False
    """

    if path.isfile(filepath):
        logger.warning(f'==> {filepath} get_checksum')
        fq = open(filepath, "rb")
        fq_contents = fq.read()
        fq.close()
        local_checksum = hashlib.md5(fq_contents).hexdigest()
        if compare_md5 == "":
            compare_md5 = local_checksum

        if compare_md5 != local_checksum:
            return False
        else:
            return local_checksum
    else:
        logger.warning(filepath + " not found.")
        return False


def parse_document(filepath):
    """This method allows local files or URLs to be parsed

    Uses basic NLP to parse a html page or file (PDF or text)
    for its contents

    Parameters
    ----------
    filepath: string
        the path to the file or URL to be scraped

    Returns:
    ----------
    tokens: list
        list of parsed tokens
    """
    full_text = ""
    tokens = []

    if filepath[0:3] == "10.1":
        # this is a doi, so just prepend and go with it
        filepath = "https://doi.org/" + filepath

    if filepath[0:3] == "htt":
        request = requests.get(filepath)
        if request.status_code == 200:
            if filepath[-3:] == "pdf":
                file_name, headers = urllib.request.urlretrieve(filepath)
                pdf_file = open(file_name, "rb")
                document = PyPDF2.PdfFileReader(pdf_file)
                for i in range(document.numPages):
                    page_to_print = document.getPage(i)
                    full_text += page_to_print.extractText()
                tokens = [
                    t for t in re.split(r"\; |\, |\. | |\n|\t", full_text)
                ]
                pdf_file.close()
            else:
                full_text = request.text
                tokens = [
                    t
                    for t in re.split(
                        r"\;|\,|\.| |\n|\t|<|>|\/|\"|'", full_text
                    )
                ]
    else:
        if filepath[-3:] == "pdf":
            pdf_file = open(filepath, "rb")
            document = PyPDF2.PdfFileReader(pdf_file)
            for i in range(document.numPages):
                page_to_print = document.getPage(i)
                full_text += page_to_print.extractText()
            tokens = [t for t in re.split(r"\; |\, |\. | |\n|\t", full_text)]
            pdf_file.close()
        else:
            text_file = open(filepath, "r")
            full_text = text_file.read()
            tokens = [
                t
                for t in re.split(r"\;|\,|\.| |\n|\t|<|>|\/|\"|'", full_text)
            ]
            text_file.close()

    return tokens


def scrape_ebi_ids(tokens, proj_id_stems=["PRJEB", "PRJNA", "ERP", "SRP"]):
    """This method allows local files or URLs to be stripped for
    EBI/ENA project IDs using tokenization

    Parameters
    ----------
    tokens: list
        list of parsed tokens from a file or webpage
    proj_id_stems: list
        list of expected project prefixes to look for

    Returns
    ---------
    found_ebi_ids : list
        list of EBI IDs found in the document
    """

    found_ebi_ids = []

    potential_tokens = {
        stem: t for stem in proj_id_stems for t in tokens if stem in t
    }
    potential_tokens = list(potential_tokens.values())

    for t in potential_tokens:
        for stem in proj_id_stems:
            # locate the start of the prefix within the token if present
            start = t.find(stem)  # will return -1 if not found

            if start != -1 and len(t) >= start + len(stem) + 1:
                # if a prefix is found, see if the next character is a number
                next_char = t[start + len(stem)]
                if next_char.isnumeric():
                    test_study = t[
                        start:
                    ]  # for now, assume that the token contains the ID
                    ebi_url = (
                        "https://www.ebi.ac.uk/ena/browser/view/"
                        + test_study
                        + "&display=xml"
                    )
                    request2 = requests.get(ebi_url)
                    invalid = True
                    if request2.status_code == 200:
                        if len(test_study) >= 6:  # min size filter
                            found_study = (
                                test_study.strip()
                                .strip(",")
                                .strip(".")
                                .strip("-")
                            )
                            found_ebi_ids.append(found_study)
                            invalid = False
                    if invalid:
                        logger.warning(
                            "Found stem: "
                            + test_study
                            + " in "
                            + test_study
                            + " but EBI project URL "
                            + ebi_url
                            + " does not exist."
                        )

    return found_ebi_ids


def get_ebi_ids(study_xml_dict):
    """Simple method to retrieve the paired study and project accessions

    If the the EBI accessions are not found, will return a tuple of
    False, False

    Parameters
    -----------
    study_xml_dict: dict
        details retrieved from fetch_ebi_info

    Returns
    ----------
    study_accession, proj_accession: (string, string) or (False, False)
        stud and project accessions if found
    """
    # TODO: does not catch case where STUDY_SET and PROJECT_SET in keys
    # TODO: consider refactor with xml object instead of dict

    study_accession = False
    proj_accession = False

    if "STUDY_SET" in study_xml_dict:
        id_dict = study_xml_dict["STUDY_SET"]["STUDY"]["IDENTIFIERS"]
        study_accession = id_dict["PRIMARY_ID"]
        if "SECONDARY_ID" in id_dict:
            proj_accession = id_dict["SECONDARY_ID"]
            # need to catch for the case when there are two secondary IDs...
            # all project accessions should start with PRJ
            if type(proj_accession) == list:
                proj_accession = "".join(
                    [p for p in proj_accession if "PRJ" in p]
                )
        else:
            logger.warning("No project ID for study: " + study_accession)
            proj_accession = study_accession
    elif "PROJECT_SET" in study_xml_dict:
        id_dict = study_xml_dict["PROJECT_SET"]["PROJECT"]["IDENTIFIERS"]
        proj_accession = id_dict["PRIMARY_ID"]
        if "SECONDARY_ID" in id_dict:
            study_accession = id_dict["SECONDARY_ID"]
        else:
            logger.warning("No study ID for project: " + proj_accession)
    else:
        logger.warning("No study information found.")

    return study_accession, proj_accession


def setup_output_dir(output_dir):
    """Helper function to ensure output path exists and is valid"""

    # output_dir directory
    if output_dir[-1] != "/":
        output_dir += "/"

    if not path.exists(output_dir):
        makedirs(output_dir, exist_ok=True)  # exist_ok shouldn't be an issue?

    return output_dir


def detect_qiita_study(metadata):
    """Simple helper function to catch when a study is already in Qiita"""
    if "qiita_study_id" in metadata.columns:
        return metadata["qiita_study_id"].unique()
    else:
        return False


def parse_details(xml_dict, null_val="XXEBIXX"):
    """Method to parse study details from xml dict

    Parameters
    -----------
    study_details: dict
        dict of study details parsed from xml

    Returns
    ----------
    desc_dict: dict
        parsed details

    """

    result_dict = {
        "abstract": null_val,
        "description": null_val,
        "title": null_val,
        "seq_method": [],
    }
    logger.warning("xml_dict", xml_dict)
    logger.warning(xml_dict)
    if "STUDY_SET" in xml_dict:
        parse_dict = xml_dict["STUDY_SET"]["STUDY"]
    elif "PROJECT_SET" in xml_dict:
        parse_dict = xml_dict["PROJECT_SET"]["PROJECT"]
    elif "SAMPLE_SET" in xml_dict:
        parse_dict = xml_dict["SAMPLE_SET"]["SAMPLE"]
    else:
        parse_dict = xml_dict["RUN_SET"]["RUN"]

    logger.warning("MEEP")
    logger.warning("parse_dict")
    logger.warning(parse_dict)

    # TODO: decide if this is necessary
    #if "DESCRIPTOR" not in parse_dict.keys():
    #    logger.warning(
    #        "No DESCRIPTOR values found. Using " + null_val + " for values."
    #    )
    #    return result_dict
    #else:
    #    desc_dict = parse_dict["DESCRIPTOR"]
    desc_dict = parse_dict
    
    if len(desc_dict) > 0:
        if "STUDY_ABSTRACT" in desc_dict.keys():
            result_dict["abstract"] = desc_dict["STUDY_ABSTRACT"]

        elif "ABSTRACT" in desc_dict.keys():
            result_dict["abstract"] = desc_dict["ABSTRACT"]
        else:
            logger.warning(
                "No abstract found, using " + null_val + " for abstract"
            )

        if "STUDY_DESCRIPTION" in desc_dict.keys():
            result_dict["description"] = desc_dict["STUDY_DESCRIPTION"]
        elif "DESCRIPTION" in desc_dict.keys():
            result_dict["description"] = desc_dict["DESCRIPTION"]
        else:
            logger.warning(
                "No description found, using " + null_val + " for description"
            )

        if "@alias" in parse_dict.keys():
            alias = parse_dict["@alias"]
            logger.warning(
                "Found EBI alias, appending '" + alias + "' to description"
            )
            if result_dict["description"] == null_val:
                result_dict["description"] = alias
            else:
                result_dict["description"] += alias

        if "STUDY_TITLE" in desc_dict.keys():
            result_dict["title"] = desc_dict["STUDY_TITLE"]
        elif "TITLE" in desc_dict.keys():
            result_dict["title"] = desc_dict["TITLE"]
        else:
            logger.warning("No title found, using " + null_val + " for title")

        for k in result_dict.keys():
            if result_dict[k] != null_val and k != "seq_method":
                result_dict["seq_method"] += scrape_seq_method(result_dict[k])
    return result_dict


def scrape_seq_method(study_text):
    """Method to search text for relevant sequencing methods"""
    valid_methods = [
        "16s",
        "18s",
        "its1",
        "its2",
        "shotgun",
    ]
    study_text = study_text.lower()
    tokens = [t for t in re.split(r"\; |\, |\. | |\n|\t", study_text)]
    found_methods = [t for meth in valid_methods for t in tokens if meth in t]
    return found_methods


def unpack_fastq_ftp(fastq_ftp, fastq_md5, fastq_bytes, layout, sep=";"):
    """Unpacks the ftp and md5 field from EBI metadata

    Takes paired set of ebi-format fastq_ftp and fastq_md5
    string values and parses them into a dictionary to be used for
    downloading and using a checksum to validate the downloads

    Parameters
    ----------
    fastq_ftp: string
        string of semicolon separated ftp filepaths
    fastq_md5: string
        string of semicolon separated md5 checksums
    fastq_bytes: string
        string of semicolon separated file sizes
    layout: int
        number of string expected


    Returns
    ----------
    remote_dict: dict
        dict of paired ftp filepaths and md5 checksums
    """
    remote_dict = {}
    error_msg = ""
    ftp_list = fastq_ftp.split(sep)
    md5_list = fastq_md5.split(sep)
    if fastq_bytes != "not provided":
        bytes_list = [int(b) for b in fastq_bytes.split(sep)]

    if len(ftp_list) == 0:
        error_msg = (
            "No ftp files present. Check study details"
            + " as access may be restricted; skipping"
        )
    elif len(ftp_list) > 3:
        error_msg = "More than 3 read files in ftp, skipping"
    elif len(ftp_list) > layout:
        error_msg = "More read files than expected in ftp."
        # This finds the smallest file and assigns it to read_0
        min_bytes = min(bytes_list)
        read_counter = 0
        read_num = 1
        while read_counter < len(ftp_list):
            read_dict = {}
            read_dict["ftp"] = ftp_list[read_counter]
            read_dict["md5"] = md5_list[read_counter]
            if fastq_bytes != "not provided":
                read_dict["bytes"] = bytes_list[read_counter]
            read_counter += 1
            if read_dict["bytes"] == min_bytes:
                remote_dict["read_0"] = read_dict
            else:
                remote_dict["read_" + str(read_num)] = read_dict
                read_num += 1
    elif len(ftp_list) < layout:
        error_msg = "Fewer read files than expected in ftp."
    else:
        read_counter = 0
        while read_counter < len(ftp_list):
            read_dict = {}
            read_dict["ftp"] = ftp_list[read_counter]
            read_dict["md5"] = md5_list[read_counter]
            if fastq_bytes != "not provided":
                read_dict["bytes"] = bytes_list[read_counter]
            read_counter += 1
            remote_dict["read_" + str(read_counter)] = read_dict

    return remote_dict, error_msg


def remove_index_read_file(fastq_dict, layout):
    """Simple method to quickly remove and rename files to
    handle samples with index files

    Parameters
    ----------
    file_prefix: string
        file_prefix to glob

    Returns
    ----------
    None

    """
    new_fastq_dict = fastq_dict
    file_prefix = (
        fastq_dict["read1"]["fp"]
        .split("/")[-1]
        .replace(".R1.ebi.fastq.gz", "")
    )
    if layout.lower() == "single":
        expected_files = 1
    elif layout.lower() == "paired":
        expected_files = 2
    else:
        logger.warning(
            "layout: "
            + str(layout)
            + " not valid for removing index read file."
        )

    if len(fastq_dict) == expected_files:
        logger.info(
            "Expected file count matches found file count for prefix "
            + file_prefix
            + ". Skipping index read removal."
        )
    elif len(fastq_dict) <= 3:
        logger.info(
            "More reads ("
            + str(len(fastq_dict))
            + ") than expected for layout "
            + layout
            + " Attempting to identify and remove index file"
        )

        index_file = ""
        read_length_dict = {}
        for f in fastq_dict.keys():
            print(f)
            read_length = get_read_length(fastq_dict[f]["fp"])
            if read_length.isnumeric():
                read_length_dict[f] = int(read_length)
            else:
                logger.warning(
                    "Read length for "
                    + fastq_dict[f]["fp"]
                    + " is non-numeric. Setting to 0 for read check."
                )
                read_length_dict[f] = 0

        read_lengths = [int(r) for r in list(read_length_dict.values())]

        if int(np.min(read_lengths)) >= int(np.mean(read_lengths)) / 2:
            logger.warning(
                "Minimum and mean read lengths are <2-fold different for."
                + file_prefix
                + " Attempting to compare read counts instead."
            )

            read_count_dict = {}

            for f in fastq_dict.keys():
                print(f)
                read_count = get_read_count(fastq_dict[f]["fp"])
                if read_count.isnumeric():
                    read_count_dict[f] = int(read_count)
                else:
                    logger.warning(
                        "Read count for "
                        + fastq_dict[f]["fp"]
                        + " is non-numeric. Setting to 0."
                    )
                    read_count_dict[f] = 0

            read_counts = [int(r) for r in list(read_count_dict.values())]

            if int(np.min(read_counts)) == int(np.mean(read_counts)):
                # this is the case when all three reads have the same count
                logger.warning(
                    +" Could not identify index read file for"
                    + file_prefix
                    + " read counts and length same for all files."
                )
            else:
                print(read_count_dict)
                index_file = min(read_count_dict, key=read_count_dict.get)
        else:
            index_file = min(read_length_dict, key=read_length_dict.get)

        if index_file != "":
            logger.info(
                index_file
                + " identified as index for "
                + file_prefix
                + " Removing and renaming other file(s)."
            )
            remove(fastq_dict[index_file]["fp"])
            index_read_num = index_file[-1]
            if int(index_read_num) == 1:
                # typically read1 is expected to be the index read if present
                # move read2 to replace read1's filepath
                move(fastq_dict["read2"]["fp"], fastq_dict[index_file]["fp"])
                # update read1's md5 to be that of the former read2
                new_fastq_dict["read1"]["md5"] = fastq_dict["read2"]["md5"]
                if "read3" in fastq_dict.keys():
                    # move read3 to the now removed read2 filepath
                    move(fastq_dict["read3"]["fp"], fastq_dict["read2"]["fp"])
                    # update read2's md to that of the former read3
                    new_fastq_dict["read2"]["md5"] = fastq_dict["read3"][
                        "md5"
                    ]
                    # remove read3 from the dict
                    del new_fastq_dict["read3"]
            elif "read3" in fastq_dict.keys():
                # if read2 is removed and there were only two, no action needed
                # otherwise move read3 to read 2
                move(fastq_dict["read3"]["fp"], fastq_dict["read2"]["fp"])
                # update read2's md to that of the former read3
                new_fastq_dict["read2"]["md5"] = fastq_dict["read3"]["md5"]
                # remove read3 from the dict
                del new_fastq_dict["read3"]
    else:
        logger.warning(
            "Removing index read files not implemented for >3 reads."
        )
        # TODO: handle samples with 4 reads (two index files?

    return new_fastq_dict
